{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка видео с помощью Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Кейсодержатели: команда NC Group**\n",
    "> 1. Боронин Фёдор (@zipi64) - CEO команды, FULLSTACK разработчик\n",
    "> 2. Хорев Артём (@horartdev) - FULLSTACK разработчик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании мы будем анализировать видео и извлекать эмоции с лиц людей, которые появляются на кадрах видео. Мы будем использовать модель для распознавания эмоций, чтобы определить доминирующую эмоцию на каждом кадре"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка зависимостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала, нам нужно установить несколько библиотек, которые мы будем использовать в этом проекте:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install deepface opencv-python matplotlib numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте подключим наши первые библиотеки к проекту, чтобы всё работало:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как обрабатывать видео в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки видео используем библиотеку OpenCV. Первый шаг — это открыть видеофайл с помощью `cv2.VideoCapture()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = '../datasets/videos/example.mp4' \n",
    "cap = cv2.VideoCapture(video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь объект `cap` позволяет нам читать кадры из видео. Давайте прочитаем кадр из видео:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, frame = cap.read() \n",
    "print(ret, frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Иногда кадра может не оказаться - например, если мы читаем конец видео или файл повреждён. Попробуйте изменить название `video_path`, а после снова выведите `ret`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1: Посмотрим на кадры!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ниже преведена часть кода, которая отвечает за то, чтобы показывать первый кадр. По крайней мере, она должна эта делать...**\n",
    "> 1. Почини код, чтобы он работал\n",
    "> 2. Добавь проверку - если кадра не существует, то должна вывестись соответствующая надпись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(ret, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Первый кадр из видео\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### А что ещё можно делать с видео?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы можем брать какую-либо информацию из видео с помощью `cap.get()` и, наоборот, устаналивать видео собственные параметры с помощью `cat.set()`. Например, получить FPS можно следующим способом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = cap.get(cv2.CAP_PROP_FPS) \n",
    "print(fps)\n",
    "\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хочется заметить, что `cap.release()` используется для прекращения чтения объекта cap. Если мы не пропишем эту функцию и попробуем вновь прочитать cap через `cap.read()`, то мы получим не первый, а уже второй кадр"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также, можно получить и другое интересное свойство видео - количество его кадров. Это можно сделать с помощью `cv2.CAP_PROP_FRAME_COUNT`. Ну, а поделив количество кадров на fps, мы получим продолжительность видео!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ эмоций с помощью DeepFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека DeepFace позволяет анализировать лица на фото и видео. Попробуем определить эмоцию человека на одной картинке!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2: Информация о человеке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала подключим DeepFace и передадим название нашего изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "\n",
    "image_path = '../datasets/photos/example1.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем по фотографии определить такую информацию, как гендер, расу, а также возраст человека на изображении с помощью `DeepFace.analyze()`. Эта функция имеет три основных аргумента - `img_path`, который показывыает путь до изображения, `actions`, где должны быть в типе `list` написаны интересующие нас факторы. В данном случае, нам интересны `gender`, `race` или `age`, а также `enforce_detection` - параметр, который определяет, будут ли попытки найти лицо на изображении(в случае, если не найдено, выдаётся ошибка). Если установить значение False, то при отсутствии лица на изображении исключение не будет возвращаться. При этом точность анализа снизится, но исключение не будет выброшено.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ниже приведён код, который должен вывести как минимум один из пунктов: gender, race или age**\n",
    "> 1. Добавь аргументы в функцию analyze, чтобы код заработал\n",
    "> 2. Поэксперементируй с разными фотографиями\n",
    "\n",
    "P.S. Возможно, при попытке первого запуска библиотеке понадобится некоторое количество времени для загрузки. Пока оно грузится, можешь прочитать, что от тебя требуется дальше!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = DeepFace.analyze()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ещё немного информации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что у нас есть следующий список: `['sad', 'sad', 'happy', 'happy', 'happy', 'normal']`. Мы хотим сделать его понятнее - узнать, сколько раз там встречаются записи 'sad', 'happy' и 'normal'. Сделать это можно следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "emotions = ['sad', 'sad', 'happy', 'happy', 'happy', 'normal']\n",
    "emotion_counts = Counter(emotions)\n",
    "emotion_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, в такой задачи может помочь и `Dict` - если он вам удобнее, используйте именно его"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Финальная задача: Эмоциональность видео"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь будет несколько подзадач, но хорошим результатом будет считаться выполнение первой из них"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final 1: Неоптимизированный анализ видео"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь ваша задача — объединить знания о работе с видео и анализе эмоций, чтобы написать собственный код для анализа видео. Используй видео `task1.mp4`, которое находится в папке `datasets/videos`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Условие:**\n",
    "> 1. Пройдитесь по всем кадрам видео.\n",
    "> 2. Используйте DeepFace для анализа эмоций.\n",
    "> 3. Считайте, сколько раз встречается каждая эмоция.\n",
    "> 4. Вместо img_path функции analyze можно написать сам объект"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Подсказка:** Используйте структуру функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(video_path):\n",
    "    # 1. Открываем видео\n",
    "    # 2. Проходимся по каждому фрейму(придумать, как можно это сделать)\n",
    "    # 3. Анализируем эмоции для каждого кадра\n",
    "    # 4. Возвращаем статистику эмоций\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final 2: График топ 3 эмоции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постройте график топ-3 эмоций с помощью Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar()\n",
    "plt.xlabel()\n",
    "plt.ylabel()\n",
    "plt.title()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final 3: Оптимизированный анализ видео"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как мы могли заметить, наш код работает весьма медленно. Допустим, что FPS нашего видео - 60, а продолжительность - 30 секунд. Конечно, 1.800 кадров будут обрабатываться весьма долго. Усложним и оптимизируем первую задачу - теперь нужно считывать по определённому количеству кадров в секунду - это число будет задаваться в аргументе `frames_per_second` нашей функции. Используй видео `task3.mp4`, которое находится в папке `datasets/videos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_video(video_path, frames_per_second=2):\n",
    "    # 1. Открываем видео\n",
    "    # 2. Извлеките кадры из видео каждый из интервала времени(например, при frames_per_second = 2 - раз в полсекунды).\n",
    "    # 3. Анализируем эмоции для каждого кадра\n",
    "    # 4. Возвращаем статистику эмоций\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final 4: График сложного анализа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте 5 графиков для осей эмоция/время. Также выведите горизонтальную столбчатую диаграмму, отражающую среднее значение каждой эмоции на видео. Подумайте, какиие визуализации также могут быть полезны, выведите их."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# код тут. Это задание под звёздочкой"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
